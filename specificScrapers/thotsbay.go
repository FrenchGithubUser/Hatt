package specificScrapers

import (
	"encoding/base64"
	"fmt"
	"hatt/assets"
	"hatt/helpers"
	"hatt/login"
	"hatt/variables"
	"io/ioutil"
	"net/http"
	"strings"
	"sync"

	"github.com/go-rod/rod"
	"github.com/go-rod/rod/lib/proto"
	"github.com/gocolly/colly"
)

// using go-rod instead of colly because the website checks on many things (headers/cookies etc) and I can't find the good combination of these to send requests without a real browser without being flagged
func (t T) Thotsbay() []variables.Item {

	var results []variables.Item
	c := colly.NewCollector()

	config := assets.DeserializeWebsiteConf("thotsbay.json")
	// serverGeneratedToken := map[string]string{
	// 	"name":  "_xfToken",
	// 	"value": "",
	// }
	// serverGeneratedToken["value"] = helpers.GetServerGeneratedTokens("https://thotsbay.ac/search/", []string{serverGeneratedToken["name"]})[serverGeneratedToken["name"]]

	login.LoginBrowser("thotsbay")

	h := &helpers.Helper{}
	tokens := h.DeserializeCredentials("thotsbay").Tokens

	// c.OnRequest(func(r *colly.Request) {
	// 	var tokensString string
	// 	for tokenName, token := range tokens {
	// 		tokensString += tokenName + "=" + token["value"] + "; "
	// 	}
	// 	tokensString = strings.TrimSuffix(tokensString, "; ")
	// 	r.Headers.Set("Cookie", tokensString)
	// 	r.Headers.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; rv:109.0) Gecko/20100101 Firefox/109.0")
	// })

	// c.OnResponse(func(r *colly.Response) {
	// 	fmt.Println(r)
	// })

	// // c.Visit(config.Search.Url + strings.ReplaceAll(variables.CURRENT_INPUT, " ", config.Search.SpaceReplacement))
	// formData := map[string]string{}
	// formData[config.Search.PostFields.Input] = variables.CURRENT_INPUT
	// formData[serverGeneratedToken["name"]] = serverGeneratedToken["value"]
	// formData["c[users]"] = ""
	// c.Post(config.Search.Url, formData)

	// send search request, a url is generated by the website with a unique id
	l := helpers.InstanciateBrowser()
	cookies := []*proto.NetworkCookieParam{}
	for tokenName, token := range tokens {
		cookies = append(cookies, &proto.NetworkCookieParam{
			Name:   tokenName,
			Value:  token["value"],
			Domain: config.SpecificInfo["domain"],
		})
	}

	browser := rod.New().ControlURL(l).MustConnect()
	browser.SetCookies(cookies)

	page := browser.MustPage(config.Search.Url)
	page.MustWaitLoad()

	page.MustElement(".inputList li:nth-of-type(1) input").MustClick().MustInput(variables.CURRENT_INPUT)

	page.MustElement(".formSubmitRow-controls button").MustClick()

	var wg sync.WaitGroup
	wg.Add(1)
	go page.EachEvent(func(e *proto.PageLoadEventFired) {
		// page loaded
		wg.Done()
	})()
	wg.Wait()

	// now that the search url has been retreived, no cookies/headers etc. are needed to view the search results, so using colly instead
	itemKeys := config.Search.ItemKeys
	c.OnHTML(itemKeys.Root, func(h *colly.HTMLElement) {

		item := variables.Item{
			Name:      h.ChildText(itemKeys.Name),
			Thumbnail: "",
			Link:      h.Request.AbsoluteURL(h.ChildAttr(itemKeys.Link, "href")),
			Metadata:  config.Search.ItemKeys.Metadata,
		}

		if item.Name != "" {
			h.ForEach(".contentRow-minor ul li", func(index int, h *colly.HTMLElement) {
				if strings.Contains(h.Text, "Replies:") {
					item.Metadata["replies"] = h.Text
				} else if h.ChildText("time") != "" {
					item.Metadata["postedAt"] = h.ChildText("time")
				}

			})

			results = append(results, item)
		}

	})

	c.Visit(page.MustInfo().URL)

	// httpCookies := []*http.Cookie{}
	// for tokenName, token := range tokens {
	// 	httpCookies = append(httpCookies, &http.Cookie{
	// 		Name:   tokenName,
	// 		Value:  token["value"],
	// 		Domain: config.SpecificInfo["domain"],
	// 	})
	// }
	// domain := &url.URL{Scheme: config.SpecificInfo["domain"]}
	// hc.Jar.SetCookies(domain, httpCookies)

	for index, item := range results {
		wg.Add(1)
		go func(item variables.Item, index int) {
			pageCollector := colly.NewCollector()
			imgUrls := []string{}

			// todo : if the item's link contains "post-xxxx", then only check for images in the specific post and not in the whole thread
			pageCollector.OnHTML("img.bbImage", func(h *colly.HTMLElement) {
				// store urls in list, then loop over list and take the first image that works
				imgUrls = append(imgUrls, h.Attr("src"))
			})

			pageCollector.Visit(item.Link)

			hc := http.Client{}
			for _, url := range imgUrls {

				req, err := http.NewRequest("GET", url, nil)
				// for _, cookie := range httpCookies {
				// 	req.AddCookie(cookie)
				// }
				img := []byte{}

				if err != nil {
					fmt.Println("error when building request : ", err)
				}

				resp, err := hc.Do(req)
				if err != nil {
					fmt.Println("error after requesting item image : ", err)
				} else {
					img, err = ioutil.ReadAll(resp.Body)
					if err != nil {
						fmt.Println("error reading img from body : ", err)
					}
				}

				if err == nil {

					var base64Encoding string

					mimeType := http.DetectContentType(img)

					switch mimeType {
					case "image/jpeg":
						base64Encoding += "data:image/jpeg;base64,"
					case "image/png":
						base64Encoding += "data:image/png;base64,"
					}

					// Append the base64 encoded output
					base64Encoding += base64.StdEncoding.EncodeToString(img)

					results[index].Thumbnail = base64Encoding
					fmt.Println(url)
					wg.Done()
					return
				}
			}
			// if no image was retreived
			wg.Done()
			return

		}(item, index)

	}
	wg.Wait()

	return results
}
